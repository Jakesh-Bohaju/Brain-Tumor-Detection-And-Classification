{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,KFold,learning_curve,validation_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from libtsvm.estimators import TSVM\n",
    "from libtsvm.model_selection import Validator, grid_search\n",
    "from libtsvm.preprocess import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bt_dataset_t3.csv\")\n",
    "print(df.describe())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.fillna(df.mean())\n",
    "# df = df.drop(['Image','PSNR','SSIM','DC','Coarseness','TS'], axis=1)\n",
    "df = df.drop(['Image','Coarseness'], axis=1)\n",
    "df['Class'] = df['Class'].replace(to_replace =0,value =-1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(df))\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[(z < 2.1).all(axis=1)]\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop(['Class'], axis=1)\n",
    "np.sum(df['Class']==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_data.csv', index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,  y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5),TN=None,FP=None,TP=None,FN=None,mse=None,AC = None,F1=None,SE=None,SP=None,PR=None,BER=None,TA=None,fname=None):\n",
    "   \n",
    "    if axes is None:\n",
    "        fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(20, 10))\n",
    "        \n",
    "\n",
    "    \n",
    "    start = time.time() \n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,scoring='neg_mean_squared_error',\n",
    "                       return_times=True)\n",
    "    TT = time.time()-start\n",
    "    \n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "    train_scores_std = -np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "    test_scores_std = -np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "    \n",
    "    a = fname.split()\n",
    "    if a[0] == 'SVM' or a[0]=='Twin':\n",
    "        param_range = np.logspace(-5, 0, 5)\n",
    "        train_scoresv, test_scoresv = validation_curve(estimator, X, y, param_name=\"gamma\", param_range=param_range,scoring=\"accuracy\", cv=5)\n",
    "    else:\n",
    "        param_range = np.arange(1, 500, 5)\n",
    "        train_scoresv, test_scoresv = validation_curve(estimator, X, y, param_name=\"n_estimators\", param_range=param_range,scoring=\"accuracy\", cv=5)\n",
    "        \n",
    "    train_scores_meanv = np.mean(train_scoresv, axis=1)\n",
    "    train_scores_stdv = np.std(train_scoresv, axis=1)\n",
    "    test_scores_meanv = np.mean(test_scoresv, axis=1)\n",
    "    test_scores_stdv = np.std(test_scoresv, axis=1)\n",
    "    \n",
    "    print(total_time)\n",
    "    print(fit_times_mean)\n",
    "    print(train_scores_mean)\n",
    "    print(test_scores_mean)\n",
    "    avg_train = np.mean(train_scores_mean)\n",
    "    avg_test = np.mean(test_scores_mean)\n",
    "    if not os.path.exists('result.csv'):\n",
    "        columns = ['Model','TN','FP','FN','TP','Accuracy','Sensitivity','Specificity','Precision','F1_Score','MSE','Error Rate','Training Accuracy','Time']\n",
    "        df = pd.DataFrame(columns = columns)\n",
    "        lst = pd.Series({'Model':fname,'TN':TN,'FP':FP,'FN':FN,'TP':TP,'Accuracy':AC,\n",
    "                         'Sensitivity':SE,'Specificity':SP,'Precision':PR,'F1_Score':F1,\n",
    "                         'MSE':mse,'Error Rate':BER,'Training Accuracy':TA,'Time':TT})\n",
    "\n",
    "        df = df.append(lst, ignore_index=True)\n",
    "        df.to_csv('result.csv',index=False)\n",
    "    else:\n",
    "        df = pd.read_csv('result.csv')\n",
    "        lst = pd.Series({'Model':fname,'TN':TN,'FP':FP,'FN':FN,'TP':TP,'Accuracy':AC,\n",
    "                         'Sensitivity':SE,'Specificity':SP,'Precision':PR,'F1_Score':F1,\n",
    "                         'MSE':mse,'Error Rate':BER,'Training Accuracy':TA,'Time':TT})\n",
    "        df = df.append(lst, ignore_index=True)\n",
    "        df.to_csv('result.csv',index=False)\n",
    "\n",
    "\n",
    "    # Plot learning curve\n",
    "    ax1.grid()\n",
    "    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training error\")\n",
    "    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n",
    "                 label=\"Cross Validation error\")\n",
    "    ax1.legend(loc=\"best\")\n",
    "    ax1.set_title(title)\n",
    "    ax1.set_xlabel(\"Training set size\")\n",
    "    ax1.set_ylabel(\"MSE\")\n",
    "\n",
    "    # validation curve\n",
    "    lw = 2\n",
    "    ax2.grid()\n",
    "    ax2.semilogx(param_range, train_scores_meanv, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "    ax2.fill_between(param_range, train_scores_meanv - train_scores_stdv,\n",
    "                     train_scores_meanv + train_scores_stdv, alpha=0.2,\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "    ax2.semilogx(param_range, test_scores_meanv, label=\"Cross-validation score\",\n",
    "                 color=\"navy\", lw=lw)\n",
    "    ax2.fill_between(param_range, test_scores_meanv - test_scores_stdv,\n",
    "                     test_scores_meanv + test_scores_stdv, alpha=0.2,\n",
    "                     color=\"navy\", lw=lw)\n",
    "    \n",
    "    ax2.set_xlabel(\"Training set size\")\n",
    "    ax2.set_ylabel(\"Score\")\n",
    "    ax2.set_title(\"Validation Curve\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    \n",
    "    \n",
    "     # Plot n_samples vs fit_times\n",
    "    ax3.grid()\n",
    "    ax3.plot(train_sizes, fit_times_mean, 'o-')\n",
    "    ax3.fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    ax3.set_xlabel(\"Training examples\")\n",
    "    ax3.set_ylabel(\"fit_times\")\n",
    "    ax3.set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    ax4.grid()\n",
    "    ax4.plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    ax4.fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    ax4.set_xlabel(\"fit_times\")\n",
    "    ax4.set_ylabel(\"Error\")\n",
    "    ax4.set_title(\"Performance of the model\")\n",
    "\n",
    "\n",
    "    fig.savefig(fname+\".png\")\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SVM with 5 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('clf',SVC(kernel='rbf'))]\n",
    "parameters = {\n",
    "    'clf__C':[0.001,0.1,10,10e5],\n",
    "    'clf__gamma':[0.1,0.01,0.001]\n",
    "}\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=5\n",
    "grid = GridSearchCV(pipeline,param_grid=parameters,cv=cv)\n",
    "grid.fit(X_train,y_train)\n",
    "parm = grid.best_params_\n",
    "print(\"Score for %d fold : = %f\"%(cv,grid.score(X_test,y_test)))\n",
    "print(\"Parameters : \",parm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "start = time.time() \n",
    "clf = SVC(kernel='rbf', gamma=parm['clf__gamma'],C=parm['clf__C'],probability=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_test= clf.predict(X_test)\n",
    "total_time = time.time()-start\n",
    "\n",
    "y_pred_train= clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse =mean_squared_error(y_test, y_pred_test)\n",
    "Train_Accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print('Train Accuracy',Train_Accuracy)\n",
    "print('Test Accuracy',accuracy_score(y_test, y_pred_test))\n",
    "print('Total Time : ',total_time)\n",
    "print('MSE: ',mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,y_pred_test).ravel()\n",
    "print(TN, FP, FN, TP)\n",
    "SE = TP/(TP+FN)  #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "SP = TN/(TN+FP)  #specificity, selectivity or true negative rate (TNR)\n",
    "AC = (TP+TN)/(TP+TN+FP+FN)   #accuracy\n",
    "PR = TP/(TP+FP)   #precision, positive predictive value (PPV)\n",
    "BCR = 1/2*(SE+SP)   #balanced classification rate\n",
    "BER = 1-BCR   #balanced error rate\n",
    "F1 = 2*PR*SE/(PR+SE)   #F1 score\n",
    "FNR = 1-SE   #miss rate or false negative rate (FNR)\n",
    "FPR = 1-SP   #fall-out or false positive rate (FPR)\n",
    "TS = TP/(TP+FP+FN)   #Threat score (TS) or Critical Success Index (CSI)\n",
    "DC = 2*TP/(2*TP+FP+FN) #Dice Coefficient\n",
    "print(AC,F1,TS,DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = r\"Learning Curves (SVM)\"\n",
    "fname = \"SVM\"\n",
    "plot_learning_curve(clf, title, X_train, y_train,\n",
    "                    cv=cv, TN=TN,FP=FP,TP=TP,FN=FN,mse=mse,AC = AC,F1=F1,SE=SE,SP=SP,PR=PR,BER=BER,TA=Train_Accuracy,fname=fname)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PCA + SVM with 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('pca',PCA()),('clf',SVC(kernel='rbf'))]\n",
    "parameters = {\n",
    "    'pca__n_components' :[2,3,4],\n",
    "    'clf__C':[0.001,0.1,10,100,10e5],\n",
    "    'clf__gamma':[1,0.1,0.01,0.001]\n",
    "}\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=5\n",
    "grid = GridSearchCV(pipeline,param_grid=parameters,cv=cv)\n",
    "grid.fit(X_train,y_train)\n",
    "parm = grid.best_params_\n",
    "print(\"Score for %d fold : = %f\"%(cv,grid.score(X_test,y_test)))\n",
    "print(\"Parameters : \",parm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components= parm['pca__n_components']) \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "start = time.time() \n",
    "clf1 = SVC(kernel=\"rbf\", gamma=parm['clf__gamma'], C=parm['clf__C'], probability=True)\n",
    "clf1.fit(X_train,y_train)\n",
    "y_pred_test= clf1.predict(X_test)\n",
    "total_time = time.time()-start# # for cv in tqdm(range(4,6)):# # for cv in tqdm(range(4,6)):# # for cv in tqdm(range(4,6)):\n",
    "\n",
    "y_pred_train= clf1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "Train_Accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print('Train Accuracy',Train_Accuracy)\n",
    "print('Test Accuracy',accuracy_score(y_test, y_pred_test))\n",
    "print('Total Time : ',total_time)\n",
    "print('MSE: ',mean_squared_error(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,y_pred_test).ravel()\n",
    "print(TN, FP, FN, TP)\n",
    "SE = TP/(TP+FN)  #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "SP = TN/(TN+FP)  #specificity, selectivity or true negative rate (TNR)\n",
    "AC = (TP+TN)/(TP+TN+FP+FN)   #accuracy\n",
    "PR = TP/(TP+FP)   #precision, positive predictive value (PPV)\n",
    "BCR = 1/2*(SE+SP)   #balanced classification rate\n",
    "BER = 1-BCR   #balanced error rate\n",
    "F1 = 2*PR*SE/(PR+SE)   #F1 score\n",
    "FNR = 1-SE   #miss rate or false negative rate (FNR)\n",
    "FPR = 1-SP   #fall-out or false positive rate (FPR)\n",
    "TS = TP/(TP+FP+FN)   #Threat score (TS) or Critical Success Index (CSI)\n",
    "DC = 2*TP/(2*TP+FP+FN) #Dice Coefficient\n",
    "print(AC,F1,TS,DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = r\"Learning Curves (SVM with PCA)\"\n",
    "fname = \"SVM with PCA\"\n",
    "plot_learning_curve(clf1, title, X_train, y_train,  ylim=(0.7, 1.01),cv=5, TN=TN,FP=FP,TP=TP,FN=FN,mse=mse,AC = AC,F1=F1,SE=SE,SP=SP,PR=PR,BER=BER,TA=Train_Accuracy,fname=fname)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('clf2',GradientBoostingClassifier())]\n",
    "\n",
    "param_grid={'clf2__n_estimators':[10,100,1000], \n",
    "            'clf2__learning_rate': [0.1,1, 0.001, 0.01], \n",
    "            'clf2__max_depth':[3,4,5,6], \n",
    "            'clf2__min_samples_leaf':[3,5], \n",
    "            'clf2__max_features':[1.0] \n",
    "           } \n",
    "n_jobs=4 \n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=5\n",
    "grid = GridSearchCV(pipeline,param_grid=param_grid,cv=cv)\n",
    "grid.fit(X_train,y_train)\n",
    "parm = grid.best_params_\n",
    "print(\"Score for %d fold : = %f\"%(cv,grid.score(X_test,y_test)))\n",
    "print(\"Parameters : \",parm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() \n",
    "clf2 = GradientBoostingClassifier(learning_rate =parm['clf2__learning_rate'], max_depth=parm['clf2__max_depth'], max_features=parm['clf2__max_features'], min_samples_leaf=parm['clf2__min_samples_leaf'],n_estimators=parm['clf2__n_estimators'],random_state=0)\n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred_test = clf2.predict(X_test)\n",
    "total_time = time.time()-start\n",
    "\n",
    "y_pred_train = clf2.predict(X_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "Train_Accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print('Train Accuracy',Train_Accuracy)\n",
    "print('Test Accuracy',accuracy_score(y_test, y_pred_test))\n",
    "print('Total Time : ',total_time)\n",
    "print('MSE: ',mean_squared_error(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,y_pred_test).ravel()\n",
    "print(TN, FP, FN, TP)\n",
    "SE = TP/(TP+FN)  #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "SP = TN/(TN+FP)  #specificity, selectivity or true negative rate (TNR)\n",
    "AC = (TP+TN)/(TP+TN+FP+FN)   #accuracy\n",
    "PR = TP/(TP+FP)   #precision, positive predictive value (PPV)\n",
    "BCR = 1/2*(SE+SP)   #balanced classification rate\n",
    "BER = 1-BCR   #balanced error rate\n",
    "F1 = 2*PR*SE/(PR+SE)   #F1 score\n",
    "FNR = 1-SE   #miss rate or false negative rate (FNR)\n",
    "FPR = 1-SP   #fall-out or false positive rate (FPR)\n",
    "TS = TP/(TP+FP+FN)   #Threat score (TS) or Critical Success Index (CSI)\n",
    "DC = 2*TP/(2*TP+FP+FN) #Dice Coefficient\n",
    "print(AC,F1,TS,DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = r\"Learning Curves (Gradient Boosting)\"\n",
    "fname = \"Gradient Boosting\"\n",
    "plot_learning_curve(clf2, title, X_train, y_train,  ylim=(0.7, 1.01),cv=5, TN=TN,FP=FP,TP=TP,FN=FN,mse=mse,AC = AC,F1=F1,SE=SE,SP=SP,PR=PR,BER=BER,TA=Train_Accuracy,fname=fname)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting with PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('pca',PCA()),('clf3',GradientBoostingClassifier())]\n",
    "\n",
    "param_grid={'pca__n_components' :[2,3,4],\n",
    "            'clf3__n_estimators':[10,100,1000], \n",
    "            'clf3__learning_rate': [0.1,1, 0.001, 0.01], \n",
    "            'clf3__max_depth':[3,4,5,6], \n",
    "            'clf3__min_samples_leaf':[3,5], \n",
    "            'clf3__max_features':[1.0] \n",
    "           } \n",
    "n_jobs=4 \n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=5\n",
    "grid = GridSearchCV(pipeline,param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train,y_train)\n",
    "parm = grid.best_params_\n",
    "print(\"Score for %d fold : = %f\"%(cv,grid.score(X_test,y_test)))\n",
    "print(\"Parameters : \",parm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components= parm['pca__n_components']) \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "start = time.time() \n",
    "clf3 = GradientBoostingClassifier(learning_rate =parm['clf3__learning_rate'], max_depth=parm['clf3__max_depth'], max_features=parm['clf3__max_features'], min_samples_leaf=parm['clf3__min_samples_leaf'],n_estimators=parm['clf3__n_estimators'],random_state=0)\n",
    "clf3.fit(X_train,y_train)\n",
    "y_pred_test= clf3.predict(X_test)\n",
    "total_time = time.time()-start\n",
    "\n",
    "y_pred_train= clf3.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "Train_Accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print('Train Accuracy',Train_Accuracy)\n",
    "print('Test Accuracy',accuracy_score(y_test, y_pred_test))\n",
    "print('Total Time : ',total_time)\n",
    "print('MSE: ',mean_squared_error(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_test,y_pred_test).ravel()\n",
    "print(TN, FP, FN, TP)\n",
    "SE = TP/(TP+FN)  #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "SP = TN/(TN+FP)  #specificity, selectivity or true negative rate (TNR)\n",
    "AC = (TP+TN)/(TP+TN+FP+FN)   #accuracy\n",
    "PR = TP/(TP+FP)   #precision, positive predictive value (PPV)\n",
    "BCR = 1/2*(SE+SP)   #balanced classification rate\n",
    "BER = 1-BCR   #balanced error rate\n",
    "F1 = 2*PR*SE/(PR+SE)   #F1 score\n",
    "FNR = 1-SE   #miss rate or false negative rate (FNR)\n",
    "FPR = 1-SP   #fall-out or false positive rate (FPR)\n",
    "TS = TP/(TP+FP+FN)   #Threat score (TS) or Critical Success Index (CSI)\n",
    "DC = 2*TP/(2*TP+FP+FN) #Dice Coefficient\n",
    "print(AC,F1,TS,DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = r\"Learning Curves (Gradient Boosting with PCA)\"\n",
    "fname = \"Gradient Boosting with PCA\"\n",
    "plot_learning_curve(clf3, title, X_train, y_train,  ylim=(0.7, 1.01),cv=5, TN=TN,FP=FP,TP=TP,FN=FN,mse=mse,AC = AC,F1=F1,SE=SE,SP=SP,PR=PR,BER=BER,TA=Train_Accuracy,fname=fname)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twin SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your dataset\n",
    "data_path = '/home/jakesh/thesis/jaya/final_data.csv'\n",
    "sep_char = ',' # separtor character of the CSV file\n",
    "header = True # Whether the dataset has header names.\n",
    "\n",
    "dataset = DataReader(data_path, sep_char, header)\n",
    "shuffle_data = True\n",
    "normalize_data = False\n",
    "\n",
    "dataset.load_data(shuffle_data, normalize_data)\n",
    "Xts, yts, file_name = dataset.get_data()\n",
    "X_traints, X_testts,  y_traints, y_testts = train_test_split(Xts, yts, test_size=0.3, random_state=42)\n",
    "sc = StandardScaler()\n",
    "X_traints = sc.fit_transform(X_traints)\n",
    "X_testts = sc.transform(X_testts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvm_clf = TSVM(kernel='linear')\n",
    "\n",
    "val = Validator(X_traints, y_traints, ('CV', 5), tsvm_clf) # 5-fold cross-validation\n",
    "eval_method = val.choose_validator()\n",
    "\n",
    "# Step 4: Specify range of each hyper-parameter for a TSVM-based estimator.\n",
    "params = {'C1': (-2, 2), 'C2': (-2, 2), 'gamma': (-10, 2)}\n",
    "\n",
    "best_acc, best_acc_std, opt_params, clf_results = grid_search(eval_method, params)\n",
    "\n",
    "print(\"Best accuracy: %.2f+-%.2f | Optimal parameters: %s\" % (best_acc, best_acc_std,\n",
    "                                                                                  str(opt_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = opt_params['C1']\n",
    "c2 = opt_params['C2']\n",
    "gamma = opt_params['gamma']\n",
    "start = time.time() \n",
    "tsvm_clf = TSVM(kernel='linear',C1= c1, C2= c2, gamma= gamma)\n",
    "tsvm_clf.fit(X_traints,y_traints)\n",
    "y_pred_test= tsvm_clf.predict(X_testts)\n",
    "total_time = time.time()-start\n",
    "y_pred_train= tsvm_clf.predict(X_traints)\n",
    "mse =mean_squared_error(y_testts, y_pred_test)\n",
    "Train_Accuracy = accuracy_score(y_traints, y_pred_train)\n",
    "print('Train Accuracy',Train_Accuracy)\n",
    "print('Test Accuracy',accuracy_score(y_testts, y_pred_test))\n",
    "print('Total Time : ',total_time)\n",
    "print('MSE: ',mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_testts,y_pred_test).ravel()\n",
    "print(TN, FP, FN, TP)\n",
    "SE = TP/(TP+FN)  #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "SP = TN/(TN+FP)  #specificity, selectivity or true negative rate (TNR)\n",
    "AC = (TP+TN)/(TP+TN+FP+FN)   #accuracy\n",
    "PR = TP/(TP+FP)   #precision, positive predictive value (PPV)\n",
    "BCR = 1/2*(SE+SP)   #balanced classification rate\n",
    "BER = 1-BCR   #balanced error rate\n",
    "F1 = 2*PR*SE/(PR+SE)   #F1 score\n",
    "FNR = 1-SE   #miss rate or false negative rate (FNR)\n",
    "FPR = 1-SP   #fall-out or false positive rate (FPR)\n",
    "TS = TP/(TP+FP+FN)   #Threat score (TS) or Critical Success Index (CSI)\n",
    "DC = 2*TP/(2*TP+FP+FN) #Dice Coefficient\n",
    "print(AC,F1,TS,DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=5\n",
    "title = r\"Learning Curves (Twin SVM)\"\n",
    "fname = \"Twin SVM\"\n",
    "plot_learning_curve(tsvm_clf, title, X_traints, y_traints,\n",
    "                    cv=cv, TN=TN,FP=FP,TP=TP,FN=FN,mse=mse,AC = AC,F1=F1,SE=SE,SP=SP,PR=PR,BER=BER,TA=Train_Accuracy,fname=fname)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twin SVM + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components= 4) \n",
    "X_traints = pca.fit_transform(X_traints)\n",
    "X_testts = pca.transform(X_testts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvm_clf1 = TSVM(kernel='linear')\n",
    "\n",
    "val = Validator(X_traints, y_traints, ('CV', 5), tsvm_clf) # 5-fold cross-validation\n",
    "eval_method = val.choose_validator()\n",
    "\n",
    "# Step 4: Specify range of each hyper-parameter for a TSVM-based estimator.\n",
    "params = {'C1': (-2, 2), 'C2': (-2, 2), 'gamma': (-10, 2)}\n",
    "\n",
    "best_acc, best_acc_std, opt_params, clf_results = grid_search(eval_method, params)\n",
    "\n",
    "print(\"Best accuracy: %.2f+-%.2f | Optimal parameters: %s\" % (best_acc, best_acc_std,\n",
    "                                                                                  str(opt_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = opt_params['C1']\n",
    "c2 = opt_params['C2']\n",
    "gamma = opt_params['gamma']\n",
    "start = time.time() \n",
    "tsvm_clf1 = TSVM(kernel='linear',C1= c1, C2= c2, gamma= gamma)\n",
    "a=tsvm_clf1.fit(X_traints,y_traints)\n",
    "y_pred_test= tsvm_clf1.predict(X_testts)\n",
    "total_time = time.time()-start\n",
    "y_pred_train= tsvm_clf1.predict(X_traints)\n",
    "mse =mean_squared_error(y_testts, y_pred_test)\n",
    "Train_Accuracy = accuracy_score(y_traints, y_pred_train)\n",
    "print('Train Accuracy',Train_Accuracy)\n",
    "print('Test Accuracy',accuracy_score(y_testts, y_pred_test))\n",
    "print('Total Time : ',total_time)\n",
    "print('MSE: ',mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = confusion_matrix(y_testts,y_pred_test).ravel()\n",
    "print(TN, FP, FN, TP)\n",
    "SE = TP/(TP+FN)  #sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "SP = TN/(TN+FP)  #specificity, selectivity or true negative rate (TNR)\n",
    "AC = (TP+TN)/(TP+TN+FP+FN)   #accuracy\n",
    "PR = TP/(TP+FP)   #precision, positive predictive value (PPV)\n",
    "BCR = 1/2*(SE+SP)   #balanced classification rate\n",
    "BER = 1-BCR   #balanced error rate\n",
    "F1 = 2*PR*SE/(PR+SE)   #F1 score\n",
    "FNR = 1-SE   #miss rate or false negative rate (FNR)\n",
    "FPR = 1-SP   #fall-out or false positive rate (FPR)\n",
    "TS = TP/(TP+FP+FN)   #Threat score (TS) or Critical Success Index (CSI)\n",
    "DC = 2*TP/(2*TP+FP+FN) #Dice Coefficient\n",
    "print(AC,F1,TS,DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=5\n",
    "title = r\"Learning Curves (Twin SVM with PCA)\"\n",
    "fname = \"Twin SVM with PCA\"\n",
    "plot_learning_curve(tsvm_clf1, title, X_traints, y_traints,\n",
    "                    cv=cv, TN=TN,FP=FP,TP=TP,FN=FN,mse=mse,AC = AC,F1=F1,SE=SE,SP=SP,PR=PR,BER=BER,TA=Train_Accuracy,fname=fname)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "models = [\n",
    "    {\n",
    "    'label': 'SVM',\n",
    "    'model': clf1,\n",
    "},\n",
    "    {\n",
    "    'label': 'SVM with PCA',\n",
    "    'model': clf1,\n",
    "},\n",
    "{\n",
    "    'label': 'Gradient Boosting',\n",
    "    'model': clf2,\n",
    "},\n",
    "{\n",
    "    'label': 'Gradient Boosting with PCA',\n",
    "    'model': clf3,\n",
    "},\n",
    "{\n",
    "    'label': 'Twin SVM',\n",
    "    'model': tsvm_clf,\n",
    "},\n",
    "{\n",
    "    'label': 'Twin SVM',\n",
    "    'model': tsvm_clf1,\n",
    "}\n",
    "]\n",
    "for m in models:\n",
    "    model = m['model'] \n",
    "    \n",
    "    if model == tsvm_clf or model == tsvm_clf1:\n",
    "        model.fit(X_traints, y_traints) \n",
    "        y_pred=model.predict(X_testts) \n",
    "        ax=model.decision_function(X_testts)[::,0]\n",
    "        nb = normalize([ax])  \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_testts, nb[0]) \n",
    "        X_test1 = X_testts\n",
    "        y_test1 = y_testts\n",
    "        \n",
    "    else:\n",
    "        model.fit(X_train, y_train) \n",
    "        y_pred=model.predict(X_test) \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "        X_test1 = X_test\n",
    "        y_test1 = y_test\n",
    "    auc = metrics.roc_auc_score(y_test1,model.predict(X_test1))\n",
    "    plt.plot(fpr, tpr, label='%s ROC (area = %0.6f)' % (m['label'], auc))\n",
    "# plt.figure(figsize=(15,10))\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"ROC.png\")\n",
    "plt.show()   # Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(\"result.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20, 8)})\n",
    "ax = sns.barplot(x=result['Accuracy'], y=result['Model'], data=result)\n",
    "for index, row in result.iterrows():\n",
    "    ax.text(row.Accuracy,row.name, str(round(row.Accuracy*100,2))+'%', color='black', ha=\"right\")\n",
    "ax.figure.savefig(\"Accuracy Bar Plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=result['Sensitivity'], y=result['Model'], data=result)\n",
    "for index, row in result.iterrows():\n",
    "    ax.text(row.Sensitivity,row.name, str(round(row.Sensitivity*100,2))+'%', color='black', ha=\"right\")\n",
    "ax.figure.savefig(\"Sensitivity Bar Plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=result['Specificity'], y=result['Model'], data=result)\n",
    "for index, row in result.iterrows():\n",
    "    ax.text(row.Specificity,row.name, str(round(row.Specificity*100,2))+'%', color='black', ha=\"right\")\n",
    "ax.figure.savefig(\"Specificity Bar Plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=result['Precision'], y=result['Model'], data=result)\n",
    "for index, row in result.iterrows():\n",
    "    ax.text(row.Precision,row.name, str(round(row.Precision*100,2))+'%', color='black', ha=\"right\")\n",
    "ax.figure.savefig(\"Precision Bar Plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=result['F1_Score'], y=result['Model'], data=result)\n",
    "for index, row in result.iterrows():\n",
    "    ax.text(row.F1_Score,row.name, str(round(row.F1_Score*100,2))+'%', color='black', ha=\"right\")\n",
    "ax.figure.savefig(\"F1-Score Bar Plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
